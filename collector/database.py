import os
import requests
from io import BytesIO
from supabase import create_client, Client

class Database:
    def __init__(self):
        url: str = os.environ.get("SUPABASE_URL") or os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
        key: str = os.environ.get("SUPABASE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
        if not url or not key:
            raise ValueError("Supabase URL and Key must be set in environment variables")
        self.supabase: Client = create_client(url, key)

    def get_blacklist(self):
        """ë¸”ëž™ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì™€ ìºì‹±í•˜ê±°ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            res = self.supabase.table("blacklist").select("type, value").execute()
            return res.data
        except:
            return []

    def get_whitelist(self):
        """í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            res = self.supabase.table("whitelist").select("type, value, auto_publish").execute()
            return res.data
        except:
            return []

    def save_raw_post(self, data: dict):
        """í¬ìŠ¤íŠ¸ë¥¼ ì €ìž¥í•˜ë©° ë¸”ëž™ë¦¬ìŠ¤íŠ¸/í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        source = data.get('source')
        source_id = data.get('source_id')
        source_url = data.get('source_url')
        
        # 1. ë¸”ëž™ë¦¬ìŠ¤íŠ¸ ì²´í¬
        blacklist = self.get_blacklist()
        for item in blacklist:
            if item['value'] in [source_id, source_url]:
                print(f"ðŸš« [Blacklist] Skipping blacklisted source: {item['value']}")
                return None

        # 2. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬ (ìžë™ ìŠ¹ì¸)
        whitelist = self.get_whitelist()
        is_whitelisted = False
        auto_pub = False
        for item in whitelist:
            if item['value'] in [source_id, source_url]:
                is_whitelisted = True
                auto_pub = item.get('auto_publish', False)
                break

        # 3. ê¸°ë³¸ ì €ìž¥ (raw_posts)
        status = "PUBLISHED" if (is_whitelisted and auto_pub) else "COLLECTED"
        data['status'] = status
        
        try:
            # ì¤‘ë³µ ì²´í¬ (URL ê¸°ë°˜)
            if self.check_duplicate(source_url):
                print(f"â­ï¸ [Duplicate] Skipping: {source_url}")
                return None

            response = self.supabase.table("raw_posts").upsert(data, on_conflict="source, source_id").execute()
            
            # 4. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìžë™ ìŠ¹ì¸ ì‹œ events í…Œì´ë¸” ë°”ë¡œ ìž…ë ¥
            if is_whitelisted and auto_pub:
                print(f"âš¡ [Whitelist] Auto-publishing: {data.get('content', {}).get('title')}")
                # ì´ ì‹œì ì—ëŠ” AI ìš”ì•½ì´ ì•„ì§ ì•ˆ ë˜ì—ˆì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ì£¼ ìˆ˜ì§‘ ë¡œì§ì—ì„œ ì²˜ë¦¬ ê¶Œìž¥ë˜ë‚˜, 
                # ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ í•„ë“œë¥¼ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
                event_data = {
                    "title": data['content'].get('title', 'ì œëª© ì—†ìŒ'),
                    "description": data['content'].get('description', data['content'].get('text', '')),
                    "category": data['content'].get('ai_suggestion', {}).get('category', 'í–‰ì‚¬'),
                    "region": data['content'].get('ai_suggestion', {}).get('region', 'ê²½ë‚¨'),
                    "poster_image_url": data.get('image_urls', [None])[0],
                    "source": source,
                    "original_url": source_url,
                    "status": "PUBLISHED",
                    "raw_post_id": response.data[0]['id'] if response.data else None
                }
                self.supabase.table("events").insert([event_data]).execute()

            return response
        except Exception as e:
            print(f"Error in save_raw_post: {e}")
            return None

    def upload_image(self, image_url: str, filename: str) -> str:
        """ì™¸ë¶€ ì´ë¯¸ì§€ URLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Supabase Storageì— ì—…ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        import requests
        from io import BytesIO
        try:
            response = requests.get(image_url, timeout=10)
            if response.status_code != 200:
                return image_url # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
            
            image_data = BytesIO(response.content)
            storage_path = f"posters/{filename}"
            
            # ì—…ë¡œë“œ
            self.supabase.storage.from_("posters").upload(
                path=storage_path,
                file=image_data.getvalue(),
                file_options={"content-type": response.headers.get("content-type", "image/jpeg")}
            )
            
            # ê³µê°œ URL ë°˜í™˜
            public_url = self.supabase.storage.from_("posters").get_public_url(storage_path)
            return public_url
        except Exception as e:
            print(f"Image upload error: {e}")
            return image_url

    def get_crawler_configs(self):
        """í™œì„±í™”ëœ í¬ë¡¤ë§ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (í˜„ìž¬ëŠ” whitelist ê¸°ë°˜ìœ¼ë¡œ í†µí•© ì‚¬ìš©)"""
        # crawler_configs í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (main.pyì—ì„œ whitelistë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„)
        return []

    def check_duplicate(self, source_url: str) -> bool:
        try:
            response = self.supabase.table("raw_posts").select("id").eq("source_url", source_url).execute()
            return len(response.data) > 0
        except Exception:
            return False

    def save_crawl_log(self, log: dict):
        """Save a crawl log entry.
        Expected keys: target_name, started_at (ISO), finished_at (ISO), result_summary, error_msg (optional).
        """
        try:
            self.supabase.table("crawl_logs").insert([log]).execute()
        except Exception as e:
            print(f"Error saving crawl log: {e}")

    def get_crawl_logs(self, limit: int = 20):
        """Fetch recent crawl logs ordered by started_at desc."""
        try:
            res = self.supabase.table("crawl_logs").select("*").order("started_at", {"ascending": false}).limit(limit).execute()
            return res.data
        except Exception as e:
            print(f"Error fetching crawl logs: {e}")
            return []
