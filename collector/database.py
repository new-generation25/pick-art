import os
import requests
from io import BytesIO
from supabase import create_client, Client

class Database:
    def __init__(self):
        url: str = os.environ.get("SUPABASE_URL") or os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
        key: str = os.environ.get("SUPABASE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
        if not url or not key:
            raise ValueError("Supabase URL and Key must be set in environment variables")
        self.supabase: Client = create_client(url, key)

    def get_blacklist(self):
        """ë¸”ëž™ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì™€ ìºì‹±í•˜ê±°ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            res = self.supabase.table("blacklist").select("type, value").execute()
            return res.data
        except:
            return []

    def get_whitelist(self):
        """í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            res = self.supabase.table("whitelist").select("type, value, auto_publish").execute()
            return res.data
        except:
            return []

    def save_raw_post(self, data: dict):
        """í¬ìŠ¤íŠ¸ë¥¼ ì €ìž¥í•˜ë©° ë¸”ëž™ë¦¬ìŠ¤íŠ¸/í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        source = data.get('source')
        source_id = data.get('source_id')
        source_url = data.get('source_url')
        title = data.get('content', {}).get('title', 'N/A')

        print(f"        ðŸ’¾ [DB] save_raw_post called for: {title[:30]}...", flush=True)
        print(f"        ðŸ’¾ [DB] Source: {source}, URL: {source_url}", flush=True)

        # 1. ë¸”ëž™ë¦¬ìŠ¤íŠ¸ ì²´í¬
        blacklist = self.get_blacklist()
        for item in blacklist:
            if item['value'] in [source_id, source_url]:
                print(f"        ðŸš« [DB:Blacklist] Blocked: {item['value']}", flush=True)
                return None

        # 2. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬ (ìžë™ ìŠ¹ì¸)
        whitelist = self.get_whitelist()
        is_whitelisted = False
        auto_pub = False
        for item in whitelist:
            wl_value = item['value']
            # ë‹¤ì–‘í•œ ë§¤ì¹­ ì „ëžµ ì‚¬ìš©
            is_match = False

            # ì „ëžµ 1: ì •í™•í•œ ë§¤ì¹­ (source_id ë˜ëŠ” source_url ì •í™•ížˆ ì¼ì¹˜)
            if wl_value in [source_id, source_url]:
                is_match = True
                print(f"        ðŸŽ¯ [DB:Whitelist] Exact match: {wl_value}", flush=True)

            # ì „ëžµ 2: URL ë„ë©”ì¸ ê¸°ë°˜ ë§¤ì¹­ (ê°™ì€ ë„ë©”ì¸ì´ë©´ í—ˆìš©)
            elif source_url and wl_value:
                # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ (https://wgcc.ghct.or.kr/... â†’ wgcc.ghct.or.kr)
                from urllib.parse import urlparse
                try:
                    wl_domain = urlparse(wl_value).netloc if 'http' in wl_value else wl_value
                    source_domain = urlparse(source_url).netloc if 'http' in source_url else source_url

                    # ë„ë©”ì¸ì´ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ë©´ ë§¤ì¹­
                    if wl_domain and source_domain and (wl_domain == source_domain or wl_domain in source_domain or source_domain in wl_domain):
                        is_match = True
                        print(f"        ðŸŒ [DB:Whitelist] Domain match: {wl_domain} in {source_domain}", flush=True)
                    # URL ê²½ë¡œ ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: show01_01 í¬í•¨ í™•ì¸)
                    elif wl_value and any(part in source_url for part in wl_value.split('/') if part and part not in ['http:', 'https:', '']):
                        is_match = True
                        print(f"        ðŸ”— [DB:Whitelist] Path component match: {wl_value}", flush=True)
                except:
                    pass

            if is_match:
                is_whitelisted = True
                auto_pub = item.get('auto_publish', False)
                print(f"        âœ… [DB:Whitelist] Matched: {item['value']}, auto_publish={auto_pub}", flush=True)
                break

        # 3. ê¸°ë³¸ ì €ìž¥ (raw_posts)
        status = "PUBLISHED" if (is_whitelisted and auto_pub) else "COLLECTED"
        data['status'] = status
        print(f"        ðŸ“‹ [DB] Status determined: {status}", flush=True)

        try:
            # ì¤‘ë³µ ì²´í¬ (URL ê¸°ë°˜)
            if self.check_duplicate(source_url):
                print(f"        â­ï¸ [DB:Duplicate] Already exists: {source_url}", flush=True)
                return None

            print(f"        ðŸ’¾ [DB] Upserting to raw_posts table...", flush=True)
            response = self.supabase.table("raw_posts").upsert(data, on_conflict="source, source_id").execute()

            if response.data:
                saved_id = response.data[0].get('id')
                print(f"        âœ… [DB] Saved to raw_posts: ID={saved_id}, Status={status}", flush=True)
            else:
                print(f"        âš ï¸ [DB] Upsert returned no data", flush=True)

            # 4. ìžë™ ìŠ¹ì¸ ë¡œì§ ë¹„í™œì„±í™” - ëª¨ë“  ë°ì´í„°ëŠ” ì¸ë°•ìŠ¤(ê²€í†  ëŒ€ê¸°)ë¡œ ë¨¼ì € ì´ë™
            # ì´ì „: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ + auto_publish=true ì‹œ ë°”ë¡œ events í…Œì´ë¸”ì— ì‚½ìž…
            # ë³€ê²½: ëª¨ë“  ë°ì´í„°ëŠ” raw_postsì— COLLECTED ìƒíƒœë¡œ ì €ìž¥ â†’ ê´€ë¦¬ìž ìŠ¹ì¸ í›„ eventsë¡œ ì´ë™
            if is_whitelisted and auto_pub:
                print(f"        â„¹ï¸ [DB:Auto-publish] Disabled - data will go to inbox for review", flush=True)
                # ìžë™ ë°œí–‰ ë¹„í™œì„±í™”ë¨ - ì•„ëž˜ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
                # ê´€ë¦¬ìžê°€ ì¸ë°•ìŠ¤ì—ì„œ ìŠ¹ì¸í•´ì•¼ events í…Œì´ë¸”ë¡œ ì´ë™
                pass


            return response
        except Exception as e:
            print(f"        âŒ [DB:Error] save_raw_post failed: {str(e)}", flush=True)
            import traceback
            print(f"        ðŸ” [DB:Traceback] {traceback.format_exc()}", flush=True)
            return None

    def upload_image(self, image_url: str, filename: str) -> str:
        """ì™¸ë¶€ ì´ë¯¸ì§€ URLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Supabase Storageì— ì—…ë¡œë“œí•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        import requests
        from io import BytesIO
        try:
            response = requests.get(image_url, timeout=10)
            if response.status_code != 200:
                return image_url # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
            
            image_data = BytesIO(response.content)
            storage_path = f"posters/{filename}"
            
            # ì—…ë¡œë“œ
            self.supabase.storage.from_("posters").upload(
                path=storage_path,
                file=image_data.getvalue(),
                file_options={"content-type": response.headers.get("content-type", "image/jpeg")}
            )
            
            # ê³µê°œ URL ë°˜í™˜
            public_url = self.supabase.storage.from_("posters").get_public_url(storage_path)
            return public_url
        except Exception as e:
            print(f"Image upload error: {e}")
            return image_url

    def get_crawler_configs(self):
        """í™œì„±í™”ëœ í¬ë¡¤ë§ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (í˜„ìž¬ëŠ” whitelist ê¸°ë°˜ìœ¼ë¡œ í†µí•© ì‚¬ìš©)"""
        # crawler_configs í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (main.pyì—ì„œ whitelistë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„)
        return []

    def check_duplicate(self, source_url: str) -> bool:
        try:
            response = self.supabase.table("raw_posts").select("id").eq("source_url", source_url).execute()
            return len(response.data) > 0
        except Exception:
            return False

    def save_crawl_log(self, log: dict):
        """Save a crawl log entry.
        Expected keys: target_name, started_at (ISO), finished_at (ISO), result_summary, error_msg (optional).
        """
        try:
            self.supabase.table("crawl_logs").insert([log]).execute()
        except Exception as e:
            print(f"Error saving crawl log: {e}")

    def get_crawl_logs(self, limit: int = 20):
        """Fetch recent crawl logs ordered by started_at desc."""
        try:
            res = self.supabase.table("crawl_logs").select("*").order("started_at", {"ascending": false}).limit(limit).execute()
            return res.data
        except Exception as e:
            print(f"Error fetching crawl logs: {e}")
            return []
