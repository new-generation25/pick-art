import aiohttp
from bs4 import BeautifulSoup
from database import Database
import asyncio
import random
from urllib.parse import urljoin
from ai_extractor import AIExtractor
from datetime import datetime

class PublicScraper:
    def __init__(self, db: Database):
        self.db = db

    async def scrape(self, dynamic_targets):
        if not dynamic_targets: return

        async with aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}) as session:
            for target in dynamic_targets:
                target_name = target.get('name', 'unknown')
                original_url = target.get('value') or target.get('url')
                log_id = target.get('log_id')
                
                # ì„œë¶€ë¬¸í™”ì„¼í„° URL ë³´ì • (ë³´í†µ ì˜ëª»ëœ ì£¼ì†Œê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ)
                if "wgcc.ghct.or.kr" in original_url and "board/show" not in original_url:
                    target_url = "https://wgcc.ghct.or.kr/board/show01_01" # ê³µì—° ì¼ì • ê¸°ë³¸ê°’
                else:
                    target_url = original_url

                print(f"\n{'='*60}", flush=True)
                print(f"[Scraper] ğŸ¯ Target: {target_name}", flush=True)
                print(f"[Scraper] ğŸ”— URL: {target_url}", flush=True)
                print(f"[Scraper] ğŸ“‹ Log ID: {log_id}", flush=True)
                print(f"{'='*60}", flush=True)
                
                try:
                    async with session.get(target_url, timeout=20) as response:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        is_wgcc = "wgcc.ghct.or.kr" in target_url
                        rows = []
                        
                        if is_wgcc:
                            # ì„œë¶€ë¬¸í™”ì„¼í„° íŠ¹í™”: ul.img_list li êµ¬ì¡°
                            rows = soup.select("ul.img_list li")
                            print(f"[Scraper] ğŸ›ï¸ WGCC mode activated", flush=True)
                            print(f"[Scraper] ğŸ“Š Found {len(rows)} items using selector 'ul.img_list li'", flush=True)
                        else:
                            rows = soup.select(".board_list tbody tr") or soup.select("ul.list_type li") or soup.select(".table-list tr")
                            print(f"[Scraper] ğŸ“Š Found {len(rows)} items using generic selectors", flush=True)

                        total_found = len(rows)
                        new_count = 0
                        skip_count = 0
                        error_count = 0

                        if total_found == 0:
                            summary = f"âŒ ë°ì´í„° ì—†ìŒ: ì„ íƒì ë¶ˆì¼ì¹˜ (Found 0 items)"
                            print(f"[Scraper] {summary}", flush=True)
                            self._update_log(log_id, summary, "FAIL")
                            continue

                        print(f"[Scraper] ğŸ”„ Processing {total_found} items...", flush=True)

                        for idx, row in enumerate(rows, 1):
                            try:
                                full_url = None
                                if is_wgcc:
                                    data_idx = row.attrs.get('data-idx')
                                    # URL êµ¬ì¡° ë¶„ì„: /board/detail/{ë©”ë‰´ì½”ë“œ}/{idx}?boardId={boardId}
                                    # ê¹€í•´ì„œë¶€ë¬¸í™”ì„¼í„° ë©”ë‰´ì½”ë“œëŠ” í˜„ì¬ show01_01(ê³µì—°) ë˜ëŠ” show02_01(ì „ì‹œ)
                                    menu_code = "show01_01" if "show01_01" in target_url else "show02_01"
                                    # boardIdëŠ” ë³´í†µ BOARD_000000111 ë“±ì´ë‚˜ idxë§Œ ìˆì–´ë„ ì ‘ì† ê°€ëŠ¥í•˜ë„ë¡ íŒ¨í„´ ìƒì„±
                                    if data_idx:
                                        full_url = f"https://wgcc.ghct.or.kr/board/detail/{menu_code}/{data_idx}"

                                    title_elem = row.select_one(".txt p")
                                    title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
                                    print(f"  [{idx}/{total_found}] ğŸ­ Title: {title[:50]}...", flush=True)
                                    print(f"  [{idx}/{total_found}] ğŸ”— URL: {full_url}", flush=True)
                                else:
                                    link_elem = row.select_one("a")
                                    if not link_elem: continue
                                    full_url = urljoin(target_url, link_elem.get('href'))
                                    title = link_elem.get_text(strip=True)
                                    print(f"  [{idx}/{total_found}] ğŸ“„ Title: {title[:50]}...", flush=True)

                                if not full_url or "/detail/" not in full_url:
                                    print(f"  [{idx}/{total_found}] âš ï¸ Invalid URL format, skipping", flush=True)
                                    continue

                                if self.db.check_duplicate(full_url):
                                    print(f"  [{idx}/{total_found}] â­ï¸ Duplicate, skipping", flush=True)
                                    skip_count += 1
                                    continue

                                # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
                                print(f"  [{idx}/{total_found}] ğŸ” Fetching detail page...", flush=True)
                                if await self.scrape_detail(session, full_url, target_name, title):
                                    print(f"  [{idx}/{total_found}] âœ… Successfully saved to database", flush=True)
                                    new_count += 1
                                    await asyncio.sleep(random.uniform(0.5, 1.5))
                                else:
                                    print(f"  [{idx}/{total_found}] âŒ Failed to save", flush=True)
                                    error_count += 1

                            except Exception as e:
                                print(f"  [{idx}/{total_found}] âš ï¸ Error: {str(e)}", flush=True)
                                error_count += 1

                        final_summary = f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ë°œê²¬ {total_found}ê±´ (ì‹ ê·œ {new_count}, ì¤‘ë³µ {skip_count}, ì‹¤íŒ¨ {error_count})"
                        print(f"\n{'='*60}", flush=True)
                        print(f"[Scraper] {final_summary}", flush=True)
                        print(f"{'='*60}\n", flush=True)
                        self._update_log(log_id, final_summary, "SUCCESS" if new_count > 0 or skip_count > 0 else "FAIL")
                                
                except Exception as e:
                    self._update_log(log_id, f"ì‚¬ì´íŠ¸ ì˜¤ë¥˜: {str(e)}", "FAIL")

    def _update_log(self, log_id, summary, status):
        if not log_id: return
        try:
            self.db.supabase.table("crawl_logs").update({
                "result_summary": summary, 
                "status": status,
                "finished_at": datetime.utcnow().isoformat()
            }).eq("id", log_id).execute()
        except: pass

    async def scrape_detail(self, session, url, source, title):
        try:
            print(f"      ğŸ“¥ Fetching HTML from: {url}", flush=True)
            async with session.get(url, timeout=20) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                content_div = soup.select_one(".board_view_cont") or soup.select_one(".view_content") or soup.select_one(".cont")
                description = content_div.get_text(strip=True) if content_div else ""
                print(f"      ğŸ“ Content extracted: {len(description)} characters", flush=True)

                img_elements = soup.select("img")
                img_urls = [urljoin(url, img.get('src')) for img in img_elements if img.get('src') and 'download' in img.get('src')]
                print(f"      ğŸ–¼ï¸ Found {len(img_urls)} images", flush=True)

                # ì œëª© ì •ë¦¬: [ê¸°íš], [íŠ¹ë³„] ë“±ì˜ ë¶„ë¥˜ íƒœê·¸ ì œê±°
                import re
                clean_title = re.sub(r'^\[.*?\]\s*', '', title).strip()
                print(f"      âœ‚ï¸ Title cleaned: '{title}' â†’ '{clean_title}'", flush=True)

                print(f"      ğŸ¤– Running AI metadata extraction...", flush=True)
                ai = AIExtractor()
                ai_suggestion = await ai.extract_metadata(f"ì œëª©: {clean_title}\në³¸ë¬¸: {description[:1000]}")

                # ì„œë¶€ë¬¸í™”ì„¼í„°ëŠ” ì§€ì—­ê³¼ ì¹´í…Œê³ ë¦¬ ê°•ì œ ì„¤ì •
                is_wgcc = 'wgcc.ghct.or.kr' in url
                if is_wgcc:
                    ai_suggestion['region'] = 'ê¹€í•´'
                    ai_suggestion['category'] = 'ê³µì—°'
                    print(f"      ğŸ›ï¸ WGCC auto-classification: region=ê¹€í•´, category=ê³µì—°", flush=True)

                print(f"      ğŸ¤– AI suggestion: category={ai_suggestion.get('category')}, region={ai_suggestion.get('region')}", flush=True)

                from utils import process_and_upload_image
                thumbnail_url = None
                p_url = None
                if img_urls:
                    print(f"      ğŸ“¤ Uploading image to storage...", flush=True)
                    t, p, _ = await process_and_upload_image(self.db.supabase, img_urls[0], f"wgcc_{random.randint(100,999)}")
                    thumbnail_url, p_url = t, p
                    print(f"      âœ… Image uploaded: thumbnail={bool(thumbnail_url)}, poster={bool(p_url)}", flush=True)

                post_data = {
                    "source": source,
                    "source_id": url.split("/")[-1],
                    "source_url": url,
                    "content": {
                        "title": clean_title,  # ì •ë¦¬ëœ ì œëª© ì‚¬ìš©
                        "description": description,
                        "ai_suggestion": ai_suggestion
                    },
                    "image_urls": [p_url] if p_url else [],
                    "status": "COLLECTED"
                }

                print(f"      ğŸ’¾ Saving to database (status: COLLECTED)...", flush=True)
                result = self.db.save_raw_post(post_data)

                if result:
                    print(f"      âœ… Database save successful - raw_posts table updated", flush=True)
                    # raw_posts í…Œì´ë¸”ì— ì €ì¥ëœ ID í™•ì¸
                    if hasattr(result, 'data') and result.data:
                        saved_id = result.data[0].get('id') if result.data else None
                        print(f"      ğŸ“Š Saved with ID: {saved_id}, Status: COLLECTED", flush=True)
                    return True
                else:
                    print(f"      âŒ Database save returned None - check blacklist/whitelist", flush=True)
                    return False

        except Exception as e:
            print(f"      âŒ Detail error ({url}): {str(e)}", flush=True)
            import traceback
            print(f"      ğŸ” Traceback: {traceback.format_exc()}", flush=True)
            return False
