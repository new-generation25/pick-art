import os
import asyncio
from dotenv import load_dotenv
from database import Database
from insta_scraper import InstaScraper
from public_scraper import PublicScraper
from fb_scraper_service import FacebookScraper
from notifier import TelegramNotifier
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime # added

# Load environment variables
load_dotenv(".env.local")

async def run_crawlers():
    print("ğŸš€ Starting crawlers with dynamic configs...")
    db = Database()
    # í¬ë¡¤ë§ ì‹œì‘ ë¡œê·¸ ê¸°ë¡ (ì´í›„ configs ë¡œë“œ ì „)
    start_time = datetime.utcnow().isoformat()
    db.save_crawl_log({
        "target_name": "all",
        "started_at": start_time,
        "finished_at": None,
        "result_summary": f"ì‹œì‘ ì „ ì„¤ì • {len([])}ê°œ",  # placeholder, will be updated after configs load
        "error_msg": None,
    })
    notifier = TelegramNotifier()
    
    try:
        # DBì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        configs = db.get_crawler_configs()
        
        # whitelist í•­ëª©ë„ í¬ë¡¤ë§ ëŒ€ìƒìœ¼ë¡œ ì¶”ê°€
        whitelist_items = db.get_whitelist()
        for item in whitelist_items:
            # type='website' ë˜ëŠ” instagram URLì¸ ê²½ìš° ì¶”ê°€
            target_type = 'website'
            if 'instagram.com' in item['value']:
                target_type = 'instagram'
            
            configs.append({
                'target_type': target_type,
                'value': item['value'],
                'name': item.get('name', 'Whitelist Source')
            })

        if not configs:
            print("âš ï¸ No active crawler configurations found.")
            return

        insta_targets = [c for c in configs if c['target_type'] == 'instagram']
        insta_targets = [c for c in configs if c['target_type'] == 'instagram']
        
        # ì›¹ì‚¬ì´íŠ¸ íƒ€ê²Ÿ ì¤‘ í˜ì´ìŠ¤ë¶ê³¼ ì¼ë°˜ ë¶„ë¦¬
        all_web_targets = [c for c in configs if c['target_type'] == 'website']
        fb_targets = [c for c in all_web_targets if 'facebook.com' in c['value']]
        web_targets = [c for c in all_web_targets if 'facebook.com' not in c['value']]

        # ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ëŸ¬ ì‹¤í–‰
        if insta_targets:
            insta = InstaScraper(db)
            # ì„¤ì •ëœ ê³„ì •/í•´ì‹œíƒœê·¸ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ë¡œì§ ìˆ˜ì • í•„ìš” (í˜„ì¬ëŠ” ë‚´ë¶€ hashtags ì‚¬ìš©)
            # await insta.scrape_targets(insta_targets) 
            await insta.scrape() # ì„ì‹œë¡œ ê¸°ì¡´ ë°©ì‹ ìœ ì§€í•˜ë˜ ë°ì´í„° ì €ì¥ ë¡œì§ì€ DB ì—°ë™ë¨
        
        # ê³µê³µê¸°ê´€ í¬ë¡¤ëŸ¬ ì‹¤í–‰
        if web_targets:
            public = PublicScraper(db)
            await public.scrape()

        # í˜ì´ìŠ¤ë¶ í¬ë¡¤ëŸ¬ ì‹¤í–‰
        if fb_targets:
            fb = FacebookScraper(db)
            await fb.scrape(fb_targets)
        
        # í¬ë¡¤ë§ ì„±ê³µ ë¡œê·¸ ê¸°ë¡
        end_time = datetime.utcnow().isoformat()
        db.save_crawl_log({
            "target_name": "all",
            "started_at": start_time,
            "finished_at": end_time,
            "result_summary": f"í¬ë¡¤ë§ ì™„ë£Œ: {len(configs)}ê°œ ì±„ë„",
            "error_msg": None,
        })
        
        await notifier.send_message(f"âœ… <b>í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ</b>\nëŒ€ìƒ: {len(configs)}ê°œ ì±„ë„\nìƒˆë¡œìš´ ë°ì´í„°ê°€ ì¸ë°•ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        # í¬ë¡¤ë§ ì‹¤íŒ¨ ë¡œê·¸ ê¸°ë¡
        end_time = datetime.utcnow().isoformat()
        db.save_crawl_log({
            "target_name": "all",
            "started_at": start_time,
            "finished_at": end_time,
            "result_summary": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {len(configs)}ê°œ ì±„ë„ ì‹œë„",
            "error_msg": str(e),
        })
        print(f"Crawler error: {e}")
        await notifier.send_error("Collector Main", str(e))
    
    print("âœ… Crawling finished.")

async def check_keyword_notifications():
    """
    ë°©ê¸ˆ ë°œí–‰ëœ ì´ë²¤íŠ¸ë“¤ì„ ì‚¬ìš©ì í‚¤ì›Œë“œì™€ ë§¤ì¹­í•˜ì—¬ ì•Œë¦¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("ğŸ” Scanning for keyword matches...")
    db = Database()
    
    try:
        # 1. ìµœê·¼ 15ë¶„ ë‚´ ë°œí–‰ëœ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        from datetime import datetime, timedelta
        import pytz
        time_threshold = (datetime.now(pytz.utc) - timedelta(minutes=15)).isoformat()
        
        events_res = db.supabase.table("events")\
            .select("id, title, description")\
            .eq("status", "PUBLISHED")\
            .gte("published_at", time_threshold)\
            .execute()
        
        new_events = events_res.data
        if not new_events:
            print("No new events to match.")
            return

        # 2. ëª¨ë“  ì‚¬ìš©ì í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keywords_res = db.supabase.table("keywords").select("*").execute()
        user_keywords = keywords_res.data
        
        notifications_count = 0
        for event in new_events:
            text_to_search = (event['title'] + " " + (event['description'] or "")).lower()
            
            for entry in user_keywords:
                keyword = entry['keyword'].lower()
                if keyword in text_to_search:
                    # ë§¤ì¹­ ì„±ê³µ! ì•Œë¦¼ ë¡œê·¸ ê¸°ë¡
                    db.supabase.table("notification_logs").insert([{
                        "email": entry['email'],
                        "event_id": event['id'],
                        "keyword": entry['keyword'],
                        "status": "SENT" # ì‹¤ì œ ë©”ì¼ ë°œì†¡ ëŒ€í–‰ ì„œë¹„ìŠ¤ ì—°ë™ ì „ ë‹¨ê³„
                    }]).execute()
                    notifications_count += 1
        
        if notifications_count > 0:
            notifier = TelegramNotifier()
            await notifier.send_message(f"ğŸ”” <b>í‚¤ì›Œë“œ ì•Œë¦¼ ìë™ ë°œì†¡</b>\në°©ê¸ˆ ë°œí–‰ëœ í–‰ì‚¬ì—ì„œ ì´ {notifications_count}ê±´ì˜ ì‚¬ìš©ì í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë°œê²¬ë˜ì–´ ì•Œë¦¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            print(f"âœ… Processed {notifications_count} keyword notifications.")

    except Exception as e:
        print(f"Error in keyword notification engine: {e}")

async def check_scheduled_events():
    """
    ì˜ˆì•½ëœ ì´ë²¤íŠ¸(SCHEDULED) ì¤‘ ì‹œê°„ì´ ëœ ê²ƒë“¤ì„ ë°œí–‰(PUBLISHED)ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
    """
    print("â° Checking for scheduled events to publish...")
    db = Database()
    from datetime import datetime
    import pytz
    
    now = datetime.now(pytz.utc)
    
    # Supabase query for SCHEDULED events where scheduled_at <= now
    # Note: Use db.supabase directly if needed or add a method to Database class
    try:
        res = db.supabase.table("events")\
            .select("id, title")\
            .eq("status", "SCHEDULED")\
            .lte("scheduled_at", now.isoformat())\
            .execute()
        
        scheduled_events = res.data
        if scheduled_events:
            for event in scheduled_events:
                db.supabase.table("events")\
                    .update({"status": "PUBLISHED", "published_at": now.isoformat()})\
                    .eq("id", event['id'])\
                    .execute()
                print(f"ğŸš€ Published scheduled event: {event['title']}")
            
            notifier = TelegramNotifier()
            await notifier.send_message(f"ğŸ“¢ <b>ì˜ˆì•½ ë°œí–‰ ì™„ë£Œ</b>\nì´ {len(scheduled_events)}ê±´ì˜ í–‰ì‚¬ê°€ ìë™ìœ¼ë¡œ ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("No events to publish at this time.")
    except Exception as e:
        print(f"Error in scheduled publisher: {e}")

async def main():
    print("Gyeongnam Art Navigator - Collector Service Started")
    
    # ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
    await run_crawlers()
    await check_scheduled_events()

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = AsyncIOScheduler()
    scheduler.add_job(run_crawlers, 'cron', hour='4,18')
    scheduler.add_job(check_scheduled_events, 'interval', minutes=10)   # 10ë¶„ë§ˆë‹¤ ì˜ˆì•½ ë°œí–‰ ì²´í¬
    scheduler.add_job(check_keyword_notifications, 'interval', minutes=10) # 10ë¶„ë§ˆë‹¤ í‚¤ì›Œë“œ ì•Œë¦¼ ì²´í¬
    scheduler.start()
    
    # ë©”ì¸ ë£¨í”„ ìœ ì§€
    try:
        while True:
            await asyncio.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        pass

if __name__ == "__main__":
    asyncio.run(main())
