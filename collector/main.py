import os
import asyncio
from dotenv import load_dotenv
from database import Database
from insta_scraper import InstaScraper
from public_scraper import PublicScraper
from fb_scraper_service import FacebookScraper
from notifier import TelegramNotifier
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime
import pytz

# Load environment variables
load_dotenv(".env.local")
notifier = TelegramNotifier()

async def run_crawlers(specific_target=None):
    """
    íŠ¹ì • íƒ€ê²Ÿ ë˜ëŠ” ì „ì²´ íƒ€ê²Ÿ í¬ë¡¤ë§ ìˆ˜í–‰
    specific_target: { 'target_type': 'website', 'value': 'url', 'name': 'name' } í˜•íƒœ
    """
    print(f"ğŸš€ {'Manual request:' if specific_target else 'Routine'} crawler started at {datetime.now()}", flush=True)
    db = Database()
    start_time = datetime.utcnow().isoformat()
    
    try:
        configs = []
        if specific_target:
            configs = [specific_target]
        else:
            # DBì—ì„œ ì •ê¸° ì„¤ì • ê°€ì ¸ì˜¤ê¸° (whitelist ê¸°ë°˜)
            whitelist_items = db.get_whitelist()
            for item in whitelist_items:
                target_type = 'website'
                if 'instagram.com' in (item.get('value') or ''):
                    target_type = 'instagram'
                
                configs.append({
                    'target_type': target_type,
                    'value': item['value'],
                    'name': item.get('name', 'Whitelist Source')
                })

        if not configs:
            print("âš ï¸ No active crawler configurations found.")
            return

        insta_targets = [c for c in configs if c.get('target_type') == 'instagram']
        all_web_targets = [c for c in configs if c.get('target_type') in ['website', 'webpage', 'source']]
        fb_targets = [c for c in all_web_targets if 'facebook.com' in (c.get('value') or '')]
        web_targets = [c for c in all_web_targets if 'facebook.com' not in (c.get('value') or '')]

        # 1. ì¸ìŠ¤íƒ€ê·¸ë¨
        if insta_targets:
            insta = InstaScraper(db)
            await insta.scrape() 
        
        # 2. ê³µê³µê¸°ê´€/í™ˆí˜ì´ì§€ (ì„œë¶€ë¬¸í™”ì„¼í„° ë“±)
        if web_targets:
            public = PublicScraper(db)
            await public.scrape(web_targets)

        # 3. í˜ì´ìŠ¤ë¶
        if fb_targets:
            fb = FacebookScraper(db)
            await fb.scrape(fb_targets)
        
        # ê²°ê³¼ ë¡œê·¸ ì—…ë°ì´íŠ¸
        end_time = datetime.utcnow().isoformat()
        log_id = specific_target.get('log_id') if specific_target else None
        
        log_data = {
            "target_name": specific_target.get('name', 'all') if specific_target else "all",
            "started_at": start_time,
            "finished_at": end_time,
            "result_summary": f"í¬ë¡¤ë§ ì™„ë£Œ: {len(configs)}ê°œ ì†ŒìŠ¤",
            "status": "SUCCESS"
        }

        if log_id:
            db.supabase.table("crawl_logs").update(log_data).eq("id", log_id).execute()
        else:
            db.save_crawl_log(log_data)
        
        # ì•Œë¦¼ (ë£¨í‹´ ì‹¤í–‰ ì‹œì—ë§Œ)
        if not specific_target:
            await notifier.send_message(f"âœ… <b>í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ</b>\nì‹ ê·œ ê²Œì‹œë¬¼ í™•ì¸ ë°”ëë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"Crawler error: {e}")
        end_time = datetime.utcnow().isoformat()
        error_log = {
            "status": "FAIL",
            "error_msg": str(e),
            "finished_at": end_time
        }
        if specific_target and specific_target.get('log_id'):
            db.supabase.table("crawl_logs").update(error_log).eq("id", specific_target['log_id']).execute()
        await notifier.send_error("Collector Main", str(e))

async def check_manual_requests():
    """
    crawl_logs í…Œì´ë¸”ì—ì„œ REQUESTED ìƒíƒœì¸ í•­ëª©ì„ ì°¾ì•„ ì¦‰ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    db = Database()
    try:
        res = db.supabase.table("crawl_logs")\
            .select("*")\
            .eq("status", "REQUESTED")\
            .order("started_at")\
            .limit(1)\
            .execute()
        
        requests = res.data
        if requests:
            req = requests[0]
            print(f"ğŸ”” Manual Request Found: {req['target_name']}")
            
            # 1. ìƒíƒœë¥¼ RUNNINGìœ¼ë¡œ ë¨¼ì € ë³€ê²½ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
            db.supabase.table("crawl_logs").update({"status": "RUNNING"}).eq("id", req['id']).execute()
            
            # 2. í•´ë‹¹ ì†ŒìŠ¤ ì •ë³´ ì°¾ê¸° (whitelistì—ì„œ value ì°¾ê¸°)
            # target_nameì´ URLì´ê±°ë‚˜ ì´ë¦„ì¼ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” value(URL)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
            source_res = db.supabase.table("whitelist").select("*").or_(f"name.eq.{req['target_name']},value.eq.{req['target_name']}").limit(1).execute()
            
            if source_res.data:
                source = source_res.data[0]
                target_type = 'website'
                if 'instagram.com' in source['value']: target_type = 'instagram'
                elif 'facebook.com' in source['value']: target_type = 'facebook'

                target_config = {
                    'target_type': target_type,
                    'value': source['value'],
                    'name': source.get('name', 'Requested Source'),
                    'log_id': req['id']
                }
                
                # 3. í¬ë¡¤ë§ ì‹¤í–‰
                await run_crawlers(specific_target=target_config)
            else:
                db.supabase.table("crawl_logs").update({
                    "status": "FAIL", 
                    "error_msg": "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }).eq("id", req['id']).execute()
                
    except Exception as e:
        print(f"Error checking manual requests: {e}")

async def main():
    print("Gyeongnam Art Navigator - Collector Service Started")
    
    # 1. ì¦‰ì‹œ ë£¨í‹´ ì‹¤í–‰
    # await run_crawlers() # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ

    # 2. ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = AsyncIOScheduler()
    scheduler.add_job(run_crawlers, 'cron', hour='4,18')
    scheduler.add_job(check_manual_requests, 'interval', seconds=30) # 30ì´ˆë§ˆë‹¤ ìˆ˜ë™ ìš”ì²­ ì²´í¬ (ì¤‘ìš”!)
    scheduler.start()
    
    # ë©”ì¸ ë£¨í”„ ìœ ì§€
    try:
        while True:
            await asyncio.sleep(10)
    except (KeyboardInterrupt, SystemExit):
        pass

if __name__ == "__main__":
    asyncio.run(main())
