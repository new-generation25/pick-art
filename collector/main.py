import os
import sys
import asyncio
from dotenv import load_dotenv
from database import Database
from insta_scraper import InstaScraper
from public_scraper import PublicScraper
from fb_scraper_service import FacebookScraper
from notifier import TelegramNotifier
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime
import pytz

# Force unbuffered output for real-time logging and UTF-8 encoding
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', line_buffering=True)
    sys.stderr.reconfigure(encoding='utf-8', line_buffering=True)
else:
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)

# Load environment variables
load_dotenv(".env.local")
notifier = TelegramNotifier()

async def run_crawlers(specific_target=None):
    print(f"ğŸš€ {'Manual request:' if specific_target else 'Routine'} crawler started at {datetime.now()}", flush=True)
    db = Database()
    start_time = datetime.utcnow().isoformat()
    
    try:
        configs = []
        if specific_target:
            configs = [specific_target]
        else:
            whitelist_items = db.get_whitelist()
            for item in whitelist_items:
                target_type = 'website'
                if 'instagram.com' in (item.get('value') or ''): target_type = 'instagram'
                configs.append({
                    'target_type': target_type,
                    'value': item['value'],
                    'name': item.get('name', 'Whitelist Source')
                })

        if not configs: return

        # ìŠ¤í¬ë˜í¼ ì‹¤í–‰ (ë¡œê·¸ëŠ” ìŠ¤í¬ë˜í¼ ë‚´ë¶€ì—ì„œ ê° log_idì— ëŒ€í•´ ìƒì„¸íˆ ê¸°ë¡í•¨)
        if specific_target and specific_target.get('target_type') == 'website':
            public = PublicScraper(db)
            await public.scrape([specific_target])
        elif not specific_target:
            # ë£¨í‹´ ì‹¤í–‰ ì‹œ
            public = PublicScraper(db)
            await public.scrape([c for c in configs if c['target_type'] == 'website'])
            # ... ë‹¤ë¥¸ ìŠ¤í¬ë˜í¼ë“¤ (FB, Insta) ...
        
        # ì „ì²´ ì¢…ë£Œ ì•Œë¦¼ (ë£¨í‹´ ì‹œì—ë§Œ)
        if not specific_target:
            await notifier.send_message(f"âœ… ì •ê¸° í¬ë¡¤ë§ ì™„ë£Œ")
        
    except Exception as e:
        print(f"Crawler error: {e}")
        if specific_target and specific_target.get('log_id'):
            db.supabase.table("crawl_logs").update({"status": "FAIL", "error_msg": str(e)}).eq("id", specific_target['log_id']).execute()

async def check_manual_requests():
    db = Database()
    try:
        # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ê°™ì€ ëŒ€ìƒì´ ì´ë¯¸ RUNNING ìƒíƒœì¸ì§€ í™•ì¸
        running_check = db.supabase.table("crawl_logs")\
            .select("target_name")\
            .eq("status", "RUNNING")\
            .execute()

        running_targets = [log['target_name'] for log in running_check.data] if running_check.data else []

        # REQUESTED ìƒíƒœ ì¤‘ RUNNINGì´ ì•„ë‹Œ ëŒ€ìƒë§Œ ì²˜ë¦¬
        res = db.supabase.table("crawl_logs")\
            .select("*")\
            .eq("status", "REQUESTED")\
            .order("started_at")\
            .limit(5)\
            .execute()

        if res.data:
            for req in res.data:
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ê±´ë„ˆë›°ê¸°
                if req['target_name'] in running_targets:
                    print(f"â­ï¸ [Manual Request] {req['target_name']} already running, skipping", flush=True)
                    # REQUESTED ìƒíƒœë¥¼ CANCELLEDë¡œ ë³€ê²½
                    db.supabase.table("crawl_logs").update({
                        "status": "CANCELLED",
                        "result_summary": "ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ì´ ìˆì–´ ì·¨ì†Œë¨",
                        "finished_at": datetime.now().isoformat()
                    }).eq("id", req['id']).execute()
                    continue

                print(f"\n{'='*60}", flush=True)
                print(f"ğŸ”” [Manual Request] Detected: {req['target_name']}", flush=True)
                print(f"ğŸ”” [Manual Request] Log ID: {req['id']}", flush=True)
                print(f"{'='*60}", flush=True)

                db.supabase.table("crawl_logs").update({"status": "RUNNING"}).eq("id", req['id']).execute()
                print(f"âœ… [Manual Request] Status updated to RUNNING", flush=True)

                source_res = db.supabase.table("whitelist")\
                    .select("*")\
                    .or_(f"name.eq.{req['target_name']},value.eq.{req['target_name']}")\
                    .limit(1)\
                    .execute()

                if source_res.data:
                    source = source_res.data[0]
                    print(f"âœ… [Manual Request] Source found in whitelist: {source.get('name')}", flush=True)
                    print(f"ğŸš€ [Manual Request] Starting crawler...\n", flush=True)
                    running_targets.append(req['target_name'])  # ì‹¤í–‰ ì¤‘ ëª©ë¡ì— ì¶”ê°€
                    await run_crawlers({
                        'target_type': 'website',
                        'value': source['value'],
                        'name': source.get('name', req['target_name']),
                        'log_id': req['id']
                    })
                else:
                    print(f"âŒ [Manual Request] Source not found in whitelist!", flush=True)
                    db.supabase.table("crawl_logs").update({
                        "status": "FAIL",
                        "error_msg": "Source not found",
                        "finished_at": datetime.now().isoformat()
                    }).eq("id", req['id']).execute()
    except Exception as e:
        print(f"âŒ [Manual Request] Error: {e}", flush=True)
        import traceback
        print(f"ğŸ” [Manual Request] Traceback: {traceback.format_exc()}", flush=True)

async def main():
    print("=" * 80, flush=True)
    print("ğŸ¨ Gyeongnam Art Navigator - Collector Service", flush=True)
    print("=" * 80, flush=True)
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print(f"ğŸ”„ Checking for manual requests every 10 seconds...", flush=True)
    print(f"ğŸ“… Scheduled crawls: 04:00 and 18:00 daily", flush=True)
    print("=" * 80 + "\n", flush=True)

    scheduler = AsyncIOScheduler()
    scheduler.add_job(run_crawlers, 'cron', hour='4,18')
    scheduler.add_job(check_manual_requests, 'interval', seconds=10) # 10ì´ˆë¡œ ë‹¨ì¶•
    scheduler.start()

    print("âœ… Scheduler started. Waiting for requests...\n", flush=True)

    while True:
        await asyncio.sleep(10)

if __name__ == "__main__":
    asyncio.run(main())
