import os
import json
import requests
from io import BytesIO
from PIL import Image
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv(".env.local")

class AIExtractor:
    def __init__(self):
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("NEXT_PUBLIC_GEMINI_API_KEY")
        if api_key:
            genai.configure(api_key=api_key)
            # Use gemini-2.5-flash for better performance
            self.model = genai.GenerativeModel('gemini-2.5-flash')
            self.enabled = True
        else:
            self.enabled = False
            print("âš ï¸ GEMINI_API_KEY not found. AI extraction disabled.")

    def _load_prompt_from_db(self):
        """Load AI prompt from Supabase configs table"""
        try:
            from supabase import create_client
            import os

            supabase_url = os.getenv("SUPABASE_URL") or os.getenv("NEXT_PUBLIC_SUPABASE_URL")
            supabase_key = os.getenv("SUPABASE_KEY") or os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY")

            if not supabase_url or not supabase_key:
                return None

            supabase = create_client(supabase_url, supabase_key)
            response = supabase.from_("configs").select("value").eq("key", "ai_prompt").single().execute()

            if response.data:
                return response.data["value"]
            return None
        except Exception as e:
            print(f"Warning: Could not load prompt from database: {e}")
            return None

    async def extract_metadata(self, text: str, image_urls: list = None):
        if not self.enabled:
            return None

        content_parts = []

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìš°ì„ ìˆœìœ„: Database > .env > fallback)
        prompt_text = self._load_prompt_from_db()

        if not prompt_text:
            prompt_text = os.getenv("AI_PROMPT")

        if not prompt_text:
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ (fallback)
            prompt_text = """Role: You are an expert Cultural Event Information Extractor specialized in Korean cultural events.

Task: Analyze the provided text and image (poster) to extract structured event data with high accuracy.

CRITICAL RULES:
1. Extract information EXACTLY as written - do not invent or guess dates/times/prices
2. If information is missing, use null (NOT "ì •ë³´ ì—†ìŒ" or empty strings)
3. Dates MUST be in YYYY-MM-DD format or null
4. Title should be clean and descriptive (remove emoji, remove source prefixes like "[í˜ì´ìŠ¤ë¶]")
5. Description should preserve original formatting and line breaks
6. Region inference priority: venue name > address > context clues

Extraction Guidelines:

**Title Extraction:**
- Remove source indicators: "[Facebook]" "[Instagram]" "@username"
- Remove excessive emoji (keep 1-2 if meaningful)
- Example: "ğŸ¨[ê²½ë‚¨ë¬¸í™”ì¬ë‹¨] ë´„ë§ì´ ì „ì‹œíšŒ" â†’ "ë´„ë§ì´ ì „ì‹œíšŒ"

**Date/Time Extraction:**
- Look for: "YYYY.MM.DD", "MMì›” DDì¼", "~ê¹Œì§€", "ë¶€í„°"
- If range: extract both date_start and date_end
- If single day: date_start = date_end
- If no date found: null (DO NOT guess)

**Venue & Region:**
- venue: Specific place name ("ê²½ë‚¨ë„ë¦½ë¯¸ìˆ ê´€", "ì„±ì‚°ì•„íŠ¸í™€")
- region: City from [ì°½ì›, ê¹€í•´, ì§„ì£¼, í†µì˜, ê±°ì œ, ì–‘ì‚°, ë°€ì–‘, í•¨ì•ˆ, ê¸°íƒ€]
- Inference: "ì°½ì› ì„±ì‚°ì•„íŠ¸í™€" â†’ region: "ì°½ì›", venue: "ì„±ì‚°ì•„íŠ¸í™€"

**Category Classification:**
Choose EXACTLY ONE from: [ì „ì‹œ, ê³µì—°, ì¶•ì œ, ì „í†µë¬¸í™”, ì²´í—˜/êµìœ¡, ê¸°íƒ€]
- ì „ì‹œ: ë¯¸ìˆ , ì‚¬ì§„, ì¡°ê° ë“± ì „ì‹œíšŒ
- ê³µì—°: ì—°ê·¹, ë®¤ì§€ì»¬, ì½˜ì„œíŠ¸, ë¬´ìš©
- ì¶•ì œ: ì§€ì—­ ì¶•ì œ, ë¬¸í™”ì œ
- ì „í†µë¬¸í™”: êµ­ì•…, ì „í†µì˜ˆìˆ 
- ì²´í—˜/êµìœ¡: ì›Œí¬ìƒµ, ê°•ì—°, êµìœ¡ í”„ë¡œê·¸ë¨

**Price Information:**
- is_free: true if "ë¬´ë£Œ", "ì…ì¥ë£Œ ì—†ìŒ", "free"
- price_details: Exact text ("5,000ì›", "ì„±ì¸ 10,000ì› / ì²­ì†Œë…„ 5,000ì›")

**Tags Generation:**
Create 5 relevant hashtags:
- 1 region tag: "#ì°½ì›" "#ê¹€í•´"
- 1-2 category tags: "#ì „ì‹œ" "#ê³µì—°" "#ì¶•ì œ"
- 2-3 theme/audience tags: "#ê°€ì¡±" "#ì£¼ë§" "#ë¬´ë£Œ" "#í˜„ëŒ€ë¯¸ìˆ "

Required JSON Output:
{
    "title": "string",
    "description": "string",
    "category": "ì „ì‹œ|ê³µì—°|ì¶•ì œ|ì „í†µë¬¸í™”|ì²´í—˜/êµìœ¡|ê¸°íƒ€",
    "region": "ì°½ì›|ê¹€í•´|ì§„ì£¼|í†µì˜|ê±°ì œ|ì–‘ì‚°|ë°€ì–‘|í•¨ì•ˆ|ê¸°íƒ€",
    "venue": "string or null",
    "date_start": "YYYY-MM-DD or null",
    "date_end": "YYYY-MM-DD or null",
    "is_free": boolean,
    "price_details": "string or null",
    "contact": "string or null",
    "organizer": "string or null",
    "tags": ["#tag1", "#tag2", "#tag3", "#tag4", "#tag5"]
}

Return ONLY valid JSON. No markdown, no explanations, no extra text."""

        content_parts.append(prompt_text)

        if text and len(text) > 5:
            content_parts.append(f"ì›ë¬¸ í…ìŠ¤íŠ¸:\n{text}")
        else:
            content_parts.append("ì›ë¬¸ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„ì„ì— ì˜ì¡´í•˜ì„¸ìš”.")

        # ì´ë¯¸ì§€ ì²˜ë¦¬ (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©)
        if image_urls and len(image_urls) > 0:
            try:
                img_url = image_urls[0]
                # ë‹¤ìš´ë¡œë“œ
                response = requests.get(img_url, timeout=10)
                if response.status_code == 200:
                    img_data = Image.open(BytesIO(response.content))
                    content_parts.append(img_data)
                    print(f"ğŸ–¼ï¸ Analyzing image for better metadata...")
            except Exception as e:
                print(f"Failed to load image for AI analysis: {e}")

        try:
            response = self.model.generate_content(content_parts)
            clean_json = response.text.replace('```json', '').replace('```', '').strip()
            return json.loads(clean_json)
        except Exception as e:
            print(f"AI Extraction failed: {e}")
            return None
